{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 14:11:39.643478: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel, TFAutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, TFAutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import list_datasets, notebook_login,login\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Dataset #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and use dataset with original hugging face dataset format ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = load_dataset(\"emotion\")\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Extract train, test, and validation dataset from the entire dataset dictionary\n",
    "train_ds = emotions['train']\n",
    "val_ds = emotions['validation']\n",
    "test_ds = emotions['test']\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong'], 'label': [0, 0, 3]}\n",
      "\n",
      "\n",
      "['text', 'label']\n",
      "\n",
      "\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}\n",
      "\n",
      "\n",
      "['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy']\n"
     ]
    }
   ],
   "source": [
    "### View content by index\n",
    "print(train_ds[:3])\n",
    "print('\\n')\n",
    "\n",
    "### View the columns names of the dataset\n",
    "print(train_ds.column_names)\n",
    "print('\\n')\n",
    "\n",
    "### View the feature summary of the dataset\n",
    "print(train_ds.features)\n",
    "print('\\n')\n",
    "\n",
    "### View the top 5 row of the text column only\n",
    "print(train_ds['text'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and use dataset directly with pandas format ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_df = load_dataset(\"emotion\")\n",
    "\n",
    "### Directly convert huggung face dataset format into pandas dataframe with .set_format(type=\"pandas\")\n",
    "emotions_df.set_format(type=\"pandas\")\n",
    "# emotions_df.reset_format() we can use this function to reset the dataset back to hugging face datasset format\n",
    "emotions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Extract train, test, and validation dataframe from the entire dataset dictionary\n",
    "\n",
    "train_df = emotions_df['train'][:]\n",
    "val_df = emotions_df['validation'][:]\n",
    "test_df = emotions_df['test'][:]\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_string\n",
       "0                            i didnt feel humiliated      0      sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0      sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3        anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2         love\n",
       "4                               i am feeling grouchy      3        anger"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The labels are converted from string category to index\n",
    "### So we need to convert them back to string using index and .int2str( index )\n",
    "\n",
    "def label_int2str(r):\n",
    "    return emotions_df[\"train\"].features[\"label\"].int2str(r)\n",
    "\n",
    "train_df['label_string'] = train_df['label'].apply(label_int2str)\n",
    "val_df['label_string'] = val_df['label'].apply(label_int2str)\n",
    "test_df['label_string'] = test_df['label'].apply(label_int2str)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Look at emotion label distribution \n",
    "\n",
    "train_df['label_string'].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Look at if set token at word level, what's the max token/word lenght in each data\n",
    "### If max length longer than model's max input sequence size, the text will be truncated to max size only\n",
    "\n",
    "train_df['Words Per Tweet'] = train_df['text'].str.split(' ').apply(len)\n",
    "train_df.boxplot(\"Words Per Tweet\",by=\"label_string\",grid=False,showfliers=False,color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenization ###\n",
    "The problem with this method is that this method ***ignore the text and sentence structure***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're totally 38 characters in the sentence.\n",
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "### 1. Split sentence in characters\n",
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(f\"There're totally {len(tokenized_text)} characters in the sentence.\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're totally unique 20 characters in the sentence.\n",
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n",
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "### 2. Index each of the character\n",
    "###    Create a dictionary to match character to index\n",
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(f\"There're totally unique {len(token2idx)} characters in the sentence.\")\n",
    "print(tokenized_text)\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "### 3. Transform the tokenized characters into a list of integers\n",
    "input_ids = [ token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first token: T\n",
      "The index of the first token: 5\n",
      "The one-hot encoded first token: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "torch.Size([38, 20])\n",
      "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "### 4. One-hot encode the input into a 2D matrix\n",
    "\n",
    "#Convert list into pytorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids,num_classes=len(token2idx))\n",
    "\n",
    "#Examine the first character in the sentence and its one-hot encoding result\n",
    "print(f\"The first token: {tokenized_text[0]}\")\n",
    "print(f\"The index of the first token: {input_ids[0]}\")\n",
    "print(f\"The one-hot encoded first token: {one_hot_encodings[0]}\")\n",
    "print(\"\\n\")\n",
    "print(one_hot_encodings.shape)\n",
    "print(one_hot_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization ###\n",
    "The major issue with word level tokenization is ***scale issue***. In larger documentation or text, there might be ***too many possible words***. Hence, the tokenization will be extremely large. Eventually, this will also ***make the NN model extremely large and with too many parameters to learn***. Hence, one common way to solve this issue is to ***use only the top most common n ( ex: 10,000 ) words in the document***. ***The rest of the words can all be classified as \"unknown\" and replaced with \"UNK\" token***.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're totally 8 words in the sentence.\n",
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "### 1. Split sentence in words and handle signs and puncuations\n",
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "text = text.replace(\".\",\"\")\n",
    "tokenized_text = text.split(\" \")\n",
    "print(f\"There're totally {len(tokenized_text)} words in the sentence.\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're totally unique 8 words in the sentence.\n",
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP']\n",
      "{'NLP': 0, 'Tokenizing': 1, 'a': 2, 'core': 3, 'is': 4, 'of': 5, 'task': 6, 'text': 7}\n"
     ]
    }
   ],
   "source": [
    "### 2. Index each of the word\n",
    "###    Create a dictionary to match word to index\n",
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(f\"There're totally unique {len(token2idx)} words in the sentence.\")\n",
    "print(tokenized_text)\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7, 4, 2, 3, 6, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "### 3. Transform the tokenized characters into a list of integers\n",
    "input_ids = [ token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first token: Tokenizing\n",
      "The index of the first token: 1\n",
      "The one-hot encoded first token: tensor([0, 1, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "torch.Size([8, 8])\n",
      "tensor([[0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "### 4. One-hot encode the input into a 2D matrix\n",
    "\n",
    "#Convert list into pytorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids,num_classes=len(token2idx))\n",
    "\n",
    "#Examine the first character in the sentence and its one-hot encoding result\n",
    "print(f\"The first token: {tokenized_text[0]}\")\n",
    "print(f\"The index of the first token: {input_ids[0]}\")\n",
    "print(f\"The one-hot encoded first token: {one_hot_encodings[0]}\")\n",
    "print(\"\\n\")\n",
    "print(one_hot_encodings.shape)\n",
    "print(one_hot_encodings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenization ###\n",
    "This method is design to solve the scale issue of word tokenization. ***Goal is to split rare words into smaller units (ex: character level) and keep frequent words as unique entities***. How the tokenizer could split is ***learned from pre-trained corpus and models***. So to use this type of tokenizers, we would need to ***load the pre-trained model***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this exercise, we use the subword tokenizer from DistilBERT pre-trained model\n",
    "\n",
    "### Use  AutoTokenizer.from_pretrained() function, you just need to specify the name of the pre-trained model to call it\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "### Encode text and map to IDs\n",
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "### Convert IDs back to tokens\n",
    "### Everything is lower case while tokenizing\n",
    "### [CLS] token means the start of the sentence\n",
    "### [SEP] token means the start of the sentence\n",
    "### ## means this token is splitted out from the previous token\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp. [SEP]\n"
     ]
    }
   ],
   "source": [
    "### Convert tokens back to text string\n",
    "text_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 30522 vocabularies used in this pre-trained tokenizer.\n",
      "The max context size of this pre-trained tokenizer is 512.\n",
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "### Check the vocabulary size of this pre-trained tokenizer\n",
    "print(f\"There're {tokenizer.vocab_size} vocabularies used in this pre-trained tokenizer.\")\n",
    "\n",
    "### Check the pre-trained tokenizer max context size\n",
    "print(f\"The max context size of this pre-trained tokenizer is {tokenizer.model_max_length}.\")\n",
    "\n",
    "### List the names of the fields the model expect the tokenizer to pass forward\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset ###\n",
    "Use the subword tokenizer tokenize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "### Create a function to tokenize dataset\n",
    "### Because each text has different length, we use padding=True to add 0 paddings to those text with shorter length. This make sure the final length of tokens of each text is the same.\n",
    "### Because each text has different length, we use truncation=True to truncate text longer than max input sequence length. This make sure the final length of tokens of each text is the same.\n",
    "### if attention_mask=0 means this token is padding and does not have meaning.\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],padding=True,truncation=True)\n",
    "\n",
    "print(tokenize(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['text', 'label', 'input_ids', 'attention_mask'], 'validation': ['text', 'label', 'input_ids', 'attention_mask'], 'test': ['text', 'label', 'input_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "### Use the dataset.map() function to apply the tokenize() function to all elements\n",
    "### Because batch_size=None , all elements will be in a single batch\n",
    "### After encoding, there 2 new columns, input_ids and attention_mask\n",
    "\n",
    "emotions_encoded = emotions.map(tokenize,batched=True,batch_size=None)\n",
    "print(emotions_encoded.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions_encoded[\"train\"][:1]\n",
    "# tokenizer.convert_ids_to_tokens(emotions_encoded[\"train\"][:1]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformers as Feature Extractors ###\n",
    "We load the pre-trained LLM without fine-tuning based on your dataset and directly use the pre-trained model generate the hidden states from input text. We just need to use the generated hidden states as variable and train a classifier. This way we don't change anything about the pre-trained LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model with Pytorch\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Detect if there's any GPU available\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device) # Chain the model to either CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "### Load model with Tensorflow\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "tf_model = TFAutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "### Some models are only in PyTorch, but we can still load the model into tensorflow framework with small adjustment\n",
    "\n",
    "tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\",from_pt=True) # the from_pt = True parameter will help transform the weights from pytorch to tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'a', 'test', '[SEP]']\n",
      "Input tensor shape:torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "### Tokenize single text with pre-trained model tokenizer\n",
    "### The final size is [input_cnt (batch size),max_token_size]\n",
    "\n",
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors='pt') # The return_tensors='pt' argument force the model to return output in PyTorch tensor\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "print(f\"Input tensor shape:{inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
      "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
      "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
      "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
      "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
      "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 6, 768])\n",
      "Single token's embedding shape:torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "### Embed single token and extract hidden states with pre-trained LLM\n",
    "### The pre-trained model did a 768 dimension embeddings for each token.\n",
    "### The final size is [input_cnt (batch size),max_token_size, embedding_dimension]\n",
    "\n",
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "with torch.no_grad(): # Disable the auto-calculation of gradient. This is useful for inference since it reduces memory footprint\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.last_hidden_state.size())\n",
    "print(f\"Single token's embedding shape:{outputs.last_hidden_state[:,0].size()}\")\n",
    "# The way of using the first token's hidden statues as the representation of the entire text is one kind of pooling strategy to reduce dimension\n",
    "# The method is called [CLS] / start token pooling strategy. Even though we only use 1 token's embedding, it still contains information of entire sequence.\n",
    "# Other common pooling strategies are max or mean pooling, which are taking the max or mean of each element of all embeddings within the entire text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 16000/16000 [12:22<00:00, 21.54 examples/s]\n",
      "Map: 100%|| 2000/2000 [01:02<00:00, 32.21 examples/s]\n",
      "Map: 100%|| 2000/2000 [01:01<00:00, 32.40 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['text', 'label', 'input_ids', 'attention_mask', 'hidden_states']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Embed the entire dataset\n",
    "\n",
    "# Create the function that will do the embeddings and extract hidden states\n",
    "\n",
    "def extract_hidden_states(batch):\n",
    "    #place model inputs on GPU if available\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs) # This model is expecting the tokenier to passed forward the \"input_ids\" and \"attention_mask\" columns \n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    return({\"hidden_states\":last_hidden_states[:,0].cpu().numpy()}) # We need to force the output back to Numpy array because the dataset.map() need the return to be Python or Numpy object\n",
    "    # The way of using the first token's hidden statues as the representation of the entire text is one kind of pooling strategy to reduce dimension\n",
    "    # The method is called [CLS] / start token pooling strategy. Even though we only use 1 token's embedding, it still contains information of entire sequence.\n",
    "    # Other common pooling strategies are max or mean pooling, which are taking the max or mean of each element of all embeddings within the entire text.\n",
    "     \n",
    "# Because the model expect the input columns to all be Pytorch tensor, we need to convert all columns to tensor format.\n",
    "\n",
    "emotions_encoded.set_format(\"torch\",\n",
    "                            columns = [\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "# Extract hidden states\n",
    "\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states,batched=True)\n",
    "\n",
    "emotions_hidden['train'].column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The entire flow to tokenize, embed, extract hidden states for the entire dataset, using pretrained LLM model and tokenizer\n",
    "\n",
    "# 1. Load pre-trained LLM model tokenizer\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Detect if there's any GPU available\n",
    "#model_name = \"distilbert-base-uncased\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 2. Create tokenization function and tokenize the entire dataset\n",
    "\n",
    "#def tokenize(batch):\n",
    "    #return tokenizer(batch[\"text\"],padding=True,truncation=True)\n",
    "\n",
    "#dataset_encoded = dataset.map(tokenize,batched=True,batch_size=None)\n",
    "\n",
    "\n",
    "# 3. Load pre-trained LLM model for embedding\n",
    "\n",
    "# Load with Pytorch\n",
    "#model_ckpt = 'distilbert-base-uncased'\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Detect if there's any GPU available\n",
    "#model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "# or\n",
    "# Load with tensorflow\n",
    "#model_ckpt = 'distilbert-base-uncased'\n",
    "#tf_model = TFAutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "\n",
    "# 4. Create embedding function, embed the entire dataset, and extract the hidden states\n",
    "\n",
    "#def extract_hidden_states(batch):\n",
    "    #inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "\n",
    "    #with torch.no_grad():\n",
    "        #outputs = model(**inputs) # This model is expecting the tokenier to passed forward the \"input_ids\" and \"attention_mask\" columns \n",
    "        #last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    #return({\"hidden_states\":last_hidden_states[:,0].cpu().numpy()})\n",
    "\n",
    "#dataset_encoded.set_format(\"torch\",\n",
    "                           #columns = [\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "#dataset_hidden = dataset_encoded.map(extract_hidden_states,batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 768)\n"
     ]
    }
   ],
   "source": [
    "### Create a feature matrix/dataframe with all the hidden states as X and label as Y\n",
    "\n",
    "x_train = np.array(emotions_hidden['train']['hidden_states'])\n",
    "x_valid = np.array(emotions_hidden['validation']['hidden_states'])\n",
    "y_train = np.array(emotions_hidden['train']['label'])\n",
    "y_valid = np.array(emotions_hidden['validation']['label'])\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6335"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train a simple multinominal logistic regression model\n",
    "### In the example, if we make a dummy guess, always guessing the majority class, the accuracy is 0.352\n",
    "### So this model is not that bad, also given that we did not deal with the imbalance data\n",
    "\n",
    "log_clf = LogisticRegression(max_iter=3000)\n",
    "log_clf.fit(x_train,y_train)\n",
    "log_clf.score(x_valid,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning Transformers ###\n",
    "In this approach, we can fine tune and train the entire trasnformer, including the embeddings and the entire NN encoding block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Under the AutoModel moduel, there's an easy function that can not only help create a sequence to classification model but also change based LLM model easily.\n",
    "# Initialize the model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Detect if there's any GPU available\n",
    "model_ckpt = 'distilbert-base-uncased' # We need to specify the base LLM model's name that you want to use.\n",
    "num_labels = 6 # Because this is a classification issue, we need to tell the model how may classes are there.\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels=num_labels).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the performance metics\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels,preds,average=\"weighted\")\n",
    "    acc = accuracy_score(labels,preds)\n",
    "    return({'accuracy':acc,\"f1\":f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into hugging face\n",
    "# command: huggingface-cli login\n",
    "# token: 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\n",
    "# notebook_login()\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"])//batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=0.00002,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=True,\n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 1:27:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.837600</td>\n",
       "      <td>0.315081</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>0.905396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>0.219713</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.925969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.5458894271850586, metrics={'train_runtime': 5241.8614, 'train_samples_per_second': 6.105, 'train_steps_per_second': 0.095, 'total_flos': 720342861696000.0, 'train_loss': 0.5458894271850586, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  processing_class=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "# The prediction output include labels and the probability of each class. Use np.argmax(,axis=1) to get the predicted label\n",
    "\n",
    "preds_output = trainer.predict(emotions_encoded['test'])\n",
    "y_preds = np.argmax(preds_output.predictions,axis=1)\n",
    "print(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.21831665933132172,\n",
       " 'test_accuracy': 0.9145,\n",
       " 'test_f1': 0.9134024013435149,\n",
       " 'test_runtime': 72.6268,\n",
       " 'test_samples_per_second': 27.538,\n",
       " 'test_steps_per_second': 0.441}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View accuracy metrics on test data\n",
    "\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "# To better understand how good or bad our model is, we can use the model predicted logits to calculate cross entropy of each predictions and investigate which part model did the worst\n",
    "\n",
    "def forward_pass_with_label(batch,preds_output):\n",
    "    loss = cross_entropy(preds_output.predictions,batch[\"label\"]) #If training the model using Pytorch\n",
    "    pred_label = np.argmax(preds_output.predictions,axis=1) #If training the model using Keras Pytorch\n",
    "\n",
    "    #loss = cross_entropy(preds_output.logits,batch[\"label\"]) #If training the model using Keras Tensorflow\n",
    "    #pred_label = np.argmax(preds_output.logits,axis=1) #If training the model using Keras Tensorflow\n",
    "\n",
    "    return({\"loss\":loss.numpy(),\"predicted_label\":pred_label.numpy()})\n",
    "\n",
    "\n",
    "# Convert dataset back to Pytorch tensors\n",
    "\n",
    "emotions_encoded.set_format(\"torch\",columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "\n",
    "emotions_encoded['test'] = emotions_encoded['test'].map(forward_pass_with_label,batched=True,batch_size=16)\n",
    "\n",
    "\n",
    "# Show result in pandas dataframe\n",
    "\n",
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\",\"label\",\"predicted_label\",\"loss\"]\n",
    "df_test = emotions_encoded[\"test\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = df_test[\"predicted_label\"].apply(label_int2str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push and save the model onto hugging face\n",
    "\n",
    "trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "model_id = \"hcniu1998/distilbert-base-uncased-finetuned-emotion\"\n",
    "classifier = pipeline(\"text-classification\",model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The entire flow to fine-tune entire trasnformers in Pytorch\n",
    "\n",
    "# 1. Load pre-trained LLM model tokenizer\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Detect if there's any GPU available\n",
    "#model_name = \"distilbert-base-uncased\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 2. Create tokenization function and tokenize the entire dataset\n",
    "\n",
    "#def tokenize(batch):\n",
    "    #return tokenizer(batch[\"text\"],padding=True,truncation=True)\n",
    "\n",
    "#dataset_encoded = dataset.map(tokenize,batched=True,batch_size=None)\n",
    "\n",
    "\n",
    "# 3. Load pre-trained LLM model\n",
    "# Under the AutoModel moduel, there's an easy function that can not only help create a sequence to classification model but also change based LLM model easily.\n",
    "# Initialize the model\n",
    "\n",
    "#model_ckpt = 'distilbert-base-uncased' # We need to specify the base LLM model's name that you want to use.\n",
    "#num_labels = n # Because this is a classification issue, we need to tell the model how may classes are there.\n",
    "#model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels=num_labels).to(device))\n",
    "\n",
    "\n",
    "# 4. Define the performance metics\n",
    "\n",
    "#def compute_metrics(pred):\n",
    "    #labels = pred.label_ids\n",
    "    #preds = pred.predictions.argmax(-1)\n",
    "    #f1 = f1_score(labels,preds,average=\"weighted\")\n",
    "    #acc = accuracy_score(labels,preds)\n",
    "    #return({'accuracy':acc,\"f1\":f1})\n",
    "\n",
    "\n",
    "# 5.Define training arguments\n",
    "\n",
    "#batch_size = 64\n",
    "#logging_steps = len(dataset_encoded[\"train\"])//batch_size\n",
    "#model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "\n",
    "#training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  #num_train_epochs=2,\n",
    "                                  #learning_rate=0.00002,\n",
    "                                  #per_device_train_batch_size=batch_size,\n",
    "                                  #per_device_eval_batch_size=batch_size,\n",
    "                                  #weight_decay=0.01,\n",
    "                                  #eval_strategy=\"epoch\",\n",
    "                                  #disable_tqdm=False,\n",
    "                                  #logging_steps=logging_steps,\n",
    "                                  #push_to_hub=True,\n",
    "                                  #log_level=\"error\")\n",
    "\n",
    "\n",
    "# 6. Log into hugging face\n",
    "\n",
    "#notebook_login()\n",
    "\n",
    "\n",
    "# 7. Train the model\n",
    "\n",
    "#trainer = Trainer(model=model,\n",
    "                  #args=training_args,\n",
    "                  #compute_metrics=compute_metrics,\n",
    "                  #train_dataset=dataset_encoded[\"train\"],\n",
    "                  #eval_dataset=dataset_encoded[\"validation\"],\n",
    "                  #processing_class=tokenizer)\n",
    "#trainer.train()\n",
    "\n",
    "\n",
    "# 8. Predictions\n",
    "\n",
    "#preds_output = trainer.predict(dataset_encoded['test'])\n",
    "#y_preds = np.argmax(preds_output.predictions,axis=1)\n",
    "\n",
    "\n",
    "# 9. View performance metrics\n",
    "\n",
    "#preds_output.metrics\n",
    "\n",
    "\n",
    "# 10. Error analysis\n",
    "# To better understand how good or bad our model is, we can use the model predicted logits to calculate cross entropy of each predictions and investigate which part model did the worst\n",
    "\n",
    "#def forward_pass_with_label(batch,preds_output):\n",
    "\n",
    "    #loss = cross_entropy(preds_output.predictions,batch[\"label\"]) #If training the model using Pytorch\n",
    "    #pred_label = np.argmax(preds_output.predictions,axis=1) #If training the model using Pytorch\n",
    "\n",
    "    #return({\"loss\":loss.numpy(),\"predicted_label\":pred_label.numpy()})\n",
    "\n",
    "\n",
    "# Convert dataset back to Pytorch tensors\n",
    "\n",
    "#dataset_encoded.set_format(\"torch\",columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "# Compute loss\n",
    "\n",
    "#dataset_encoded['test'] = dataset_encoded['test'].map(forward_pass_with_label,batched=True,batch_size=16)\n",
    "\n",
    "\n",
    "# Show result in pandas dataframe\n",
    "\n",
    "#dataset_encoded.set_format(\"pandas\")\n",
    "#cols = [\"text\",\"label\",\"predicted_label\",\"loss\"]\n",
    "#df_test = dataset_encoded[\"test\"][:][cols]\n",
    "#df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "#df_test[\"predicted_label\"] = df_test[\"predicted_label\"].apply(label_int2str)\n",
    "\n",
    "\n",
    "# 11. Push and save the model onto hugging face\n",
    "\n",
    "#trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "\n",
    "# 12. Load the model from hugging face\n",
    "\n",
    "#model_id = \"hcniu1998/distilbert-base-uncased-finetuned-emotion\"\n",
    "#classifier = pipeline(\"text-classification\",model=model_id) # The first parameter is the pipeline_tag you can find it from the model card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haochunniu/Desktop/Python/Transformer/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x1806abeb0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x1806abeb0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "250/250 [==============================] - 2466s 10s/step - loss: 0.5161 - sparse_categorical_accuracy: 0.8232 - val_loss: 0.1666 - val_sparse_categorical_accuracy: 0.9345\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 2249s 9s/step - loss: 0.1466 - sparse_categorical_accuracy: 0.9377 - val_loss: 0.1330 - val_sparse_categorical_accuracy: 0.9440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x18ee0a560>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The entire flow to fine-tune entire trasnformers in Tensorflow\n",
    "\n",
    "# 1. Load pre-trained LLM model tokenizer\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Detect if there's any GPU available\n",
    "#model_name = \"distilbert-base-uncased\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 2. Create tokenization function and tokenize the entire dataset\n",
    "\n",
    "#def tokenize(batch):\n",
    "    #return tokenizer(batch[\"text\"],padding=True,truncation=True)\n",
    "\n",
    "#dataset_encoded = dataset.map(tokenize,batched=True,batch_size=None)\n",
    "\n",
    "\n",
    "# 3. Load pre-trained LLM model\n",
    "# Under the AutoModel moduel, there's an easy function that can not only help create a sequence to classification model but also change based LLM model easily.\n",
    "# Initialize the model\n",
    "\n",
    "#model_ckpt = 'distilbert-base-uncased' # We need to specify the base LLM model's name that you want to use.\n",
    "#num_labels = 6 # Because this is a classification issue, we need to tell the model how may classes are there.\n",
    "#tf_model = (TFAutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels=num_labels))\n",
    "\n",
    "\n",
    "# 4. Transform required columns to tensorflow\n",
    "\n",
    "#batch_size = 64\n",
    "#tokenizer_columns = tokenizer.model_input_names\n",
    "#tf_train_dataset = dataset_encoded[\"train\"].to_tf_dataset(columns=tokenizer_columns,label_cols=[\"label\"],shuffle=True,batch_size=batch_size)\n",
    "#tf_eval_dataset = dataset_encoded[\"validation\"].to_tf_dataset(columns=tokenizer_columns,label_cols=[\"label\"],shuffle=False,batch_size=batch_size)\n",
    "#tf_test_dataset = dataset_encoded[\"test\"].to_tf_dataset(columns=tokenizer_columns,label_cols=[\"label\"],shuffle=False,batch_size=batch_size)\n",
    "\n",
    "# 5. Compile and fit the model\n",
    "\n",
    "#tf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n",
    "#                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#                 metrics=tf.metrics.SparseCategoricalAccuracy()) \n",
    "\n",
    "#tf_model.fit(tf_train_dataset,validation_data=tf_eval_dataset,epochs=2)\n",
    "\n",
    "\n",
    "# 6. Predictions\n",
    "\n",
    "#preds_output = tf_model.predict(tf_test_dataset)\n",
    "#y_preds = np.argmax(preds_output.logits,axis=1)\n",
    "\n",
    "\n",
    "# 7. Error analysis\n",
    "# To better understand how good or bad our model is, we can use the model predicted logits to calculate cross entropy of each predictions and investigate which part model did the worst\n",
    "\n",
    "#def forward_pass_with_label(batch,preds_output):\n",
    "\n",
    "    #loss = cross_entropy(preds_output.logits,batch[\"label\"]) #If training the model using Keras Tensorflow\n",
    "    #pred_label = np.argmax(preds_output.logits,axis=1) #If training the model using Keras Tensorflow\n",
    "\n",
    "    #return({\"loss\":loss.numpy(),\"predicted_label\":pred_label.numpy()})\n",
    "\n",
    "\n",
    "# Convert dataset back to Pytorch tensors\n",
    "\n",
    "#dataset_encoded.set_format(\"torch\",columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "# Compute loss\n",
    "\n",
    "#dataset_encoded['test'] = dataset_encoded['test'].map(forward_pass_with_label,batched=True,batch_size=16)\n",
    "\n",
    "\n",
    "# Show result in pandas dataframe\n",
    "\n",
    "#dataset_encoded.set_format(\"pandas\")\n",
    "#cols = [\"text\",\"label\",\"predicted_label\",\"loss\"]\n",
    "#df_test = dataset_encoded[\"test\"][:][cols]\n",
    "#df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "#df_test[\"predicted_label\"] = df_test[\"predicted_label\"].apply(label_int2str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('transformer': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdc66dc44c0f502b7f5763436885fc80a7f1e66ec3e3651f083a9873f1c92c33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
