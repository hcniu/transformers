{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel, TFAutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, TFAutoModelForSequenceClassification,AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import list_datasets, notebook_login,login\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new tokens to existed pre-trained tokenizer ###\n",
    "In reality, pre-trained tokenizer might already covered the language we are using. Yet, in some special cases, we might want to ***add some special words into out tokenizer's vocabulary list***. After adding these new words, ***the tokenizer will recognize these words and split the sequence correctly***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "### Create a list of new tokens that need to be added\n",
    "new_words = np.unique(['Eric','Amy','SS'])\n",
    "\n",
    "### Call the vocabulary from the pretrained tokenizer and check if the word is already in there\n",
    "### If the word is already in the tokenizer's vocabulary, we don't need to add.\n",
    "existing_vocab = tokenizer.get_vocab()\n",
    "tokens_to_add = [tok for tok in new_words if f\"{tok}\" not in existing_vocab]\n",
    "\n",
    "### Add the tokens to tokenizer\n",
    "tokenizer.add_tokens(tokens_to_add)\n",
    "\n",
    "### Save the updated tokenizer\n",
    "tokenizer.save_pretrained(\"/home/folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the model ###\n",
    "After updating the tokenizer, the embedding shape will be different. So, ***we would need to update the model and rezise the embedding of the model***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the pre-trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "### Update the embedding size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "### Save the updated model\n",
    "model.save_pretrained(\"/home/folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
